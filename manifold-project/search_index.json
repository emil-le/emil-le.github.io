[["preface.html", "Geometry and topology statistical inference with applications. Preface", " Geometry and topology statistical inference with applications. Emil Le 2021-03-05 Preface Statistical inference has multiple applications in the field of computer vision, which dealt with interdisciplinary topics ranging from biomedical imaging, object recognition, motion tracking and so on. Intuitively, the geometry of such visual data can be one of the most important feature for solving such problems. Since the optimization of computational expenses can determine the pragmatic of implementation, the question we can ask is that how can we map the input space to lower dimensional space while preserving as much local feature as possible. This leads to the development of statistical learning on geometry and topology in order to tackle the challenge. This capstone project tries to provide an intuitive introduction with examples included in. "],["intro.html", "Chapter 1 Introduction 1.1 Homeomorphism 1.2 Differential Manifold 1.3 Probability measure", " Chapter 1 Introduction While statistical methods on Eucledian space assumes that data has inherent structure, the premise of manifold learning explicitly believes that data are in fact lying on a low-dimensional manifold embedded in a higher-dimensional space, which is called the “Manifold Hypothesis.” There are several axioms, definitions and concepts to be included in order for us to think analogously about statistical methods non-eucledian space. 1.1 Homeomorphism Let \\(\\mathcal{X}\\) be an arbitrary set and the collection \\(\\mathcal{T} = \\{\\emptyset,\\mathcal{X},\\mathcal{U_i}\\vert\\mathcal{U_i}\\subseteq \\mathcal{X}\\}\\). If every arbitrary union and finite intersection of set contained in \\(\\mathcal{T}\\) also belongs to the collect, then \\(\\mathcal{T}\\) is an topology on \\(\\mathcal{X}\\) and \\((\\mathcal{X},\\mathcal{T})\\) is a topological space. Every sets contained in the topological space is defined as “open” under their respective topology. theta = seq(1,100) x = (1-1/theta)*sin(theta) y = (1-1/theta)*cos(theta) M &lt;- mesh(x, y) x= M$x y =M$y plot(x,y,type=&#39;p&#39;) Figure 1.1: 2-Torus If there exists a bijection \\(f:\\mathcal{X}\\to\\mathcal{Y}\\) such that \\(f(U)\\) is open in topological space \\(\\mathcal{Y}\\) and \\(f^{-1}(V)\\) is open in space \\(\\mathcal{X}\\) for any arbitrary \\(U\\subseteq\\mathcal{X},V\\subset\\mathcal{Y}\\), then \\(\\mathcal{X}\\) is homeomorphic to \\(\\mathcal{Y}\\) (denote as \\(\\mathcal{X} \\cong \\mathcal{Y}\\) for \\(\\cong\\) is an equivalent relation) and \\(f\\) is a homeomorphism. An embedding of \\(\\mathcal{X}\\) in \\(\\mathcal{Y}\\) is a function \\(f: \\mathcal{X}\\to\\mathcal{Y}\\) that maps \\(\\mathcal{X}\\) homeomorphically to the subspace \\(f(\\mathcal{X})\\) in \\(\\mathcal{Y}\\). 1.2 Differential Manifold Informally, we can think of a manifold as a topological space (“shape”) that is locally resemblance to an Euclidean space, but not globally. For example, the Earth is an 2-manifold because we perceive the shape of it locally as a flat surface. This provides us a way to quantify the shape or geometric feature of visual data. Let \\(\\mathcal{M}\\) be arbitrary Hausdorff topological space with countable basis. For any arbitrary \\(x \\in \\mathcal{M}\\), if there exists an open neighborhood \\(U \\subset \\mathcal{M}\\) of \\(x\\) and a homeomorphism \\(f: U \\to \\mathbb{R}^n\\) then \\(\\mathcal{M}\\) is an n-manifold. By Urysohn theorem, it is possible to assign a metric for \\(\\mathcal{M}\\). Since the manifold is globally resemblance to an Euclidean space, the conventional calculus and probability axioms cannot be applied. Using differential geometry of the underlying manifolds, we can derive results that have local resemblance to the Euclidean space. Let \\(T_p\\mathcal{M}\\) be the tangent space of \\(\\mathcal{M}\\) and arbitrary \\(p\\), if the Riemannian metric assigns each p with positive-definite inner product \\(\\phi_p: T_p\\mathcal{M} \\times T_p\\mathcal{M} \\to \\mathbb{R}\\), and a norm \\(\\|.\\|_p: T_pM \\to \\mathbb{R}\\) such that \\(\\|v\\|_p^2 = \\phi_p(v,v)\\), then \\((\\mathcal{M},g)\\) is called a Riemannian manifold. 1.3 Probability measure "],["statistical-shape-matching.html", "Chapter 2 Statistical shape matching 2.1 Intrinsic data analysis 2.2 Central Limit Theorem 2.3 Geodesics Regression 2.4 Isomorphic embedding", " Chapter 2 Statistical shape matching 2.1 Intrinsic data analysis 2.1.1 Fréchet mean and variance Since a \\(\\mathcal{M}\\)-random variable (r.v) \\(\\mathcal{X}:(\\Omega,\\mathcal{F},\\mathbb{P})\\to\\mathcal{B}_{\\mathcal{M}}\\) is measurable, assign every \\(\\mathcal{X}\\) with measure \\(Q = \\mathbb{P}_{\\mathcal{X}}(\\mathcal{X})\\) on \\(\\mathcal{B}_{\\mathcal{M}}\\) generated by \\(Q(B)=\\mathbb{P}(\\mathcal{X}^{-1}({B}))\\) for any arbitrary \\(B\\in\\mathcal{B}_{\\mathcal{B}}\\). Recall that on Riemannian manifold, there exists \\(\\phi_p: \\mathcal{M}\\to\\mathbb{R}^n\\) contained in \\(\\mathcal{L}_{Q}^r(\\mathcal{M})\\) and an embedding \\(p:\\mathcal{M}\\to\\mathbb{R}^n\\) such that \\[\\phi_p(u,v)=\\|p(v)-p(u)\\|_2\\] Any \\(\\mathcal{F}(v):\\mathcal{M}\\to\\mathbb{R}^n\\) such that \\[\\mathcal{F}(v)=\\int \\phi^2(v,u)\\mathcal{Q}(dx)\\] is defined as the Fréchet function. Then the Fréchet mean and variance are define as \\[\\mu_{F} = \\arg\\inf_\\mu \\mathbb{E}_{\\mathcal{X}}[\\phi^2(\\mathcal{X},\\mu)]\\] \\[\\sigma^2_{F}= \\inf_{\\mu} \\mathbb{E}_{\\mathcal{X}}[d^2(\\mathcal{X},\\mu)]\\] Furthermore, \\(\\mu_{F}\\) and \\(\\sigma^2_{F}\\) is also referred as an intrinsic mean and variance, which can be understand as an attempt to make analogy to the usual standard probability space. In fact, in \\(\\mathbb{R}_{std}\\), the intrinsic mean is the expectation of r.v \\(X\\). Intuitively, this may suggest an extrinsic mean (which it is indeed true). Lemma 2.1 If the curvature \\((\\mathcal{M},\\phi)\\) is non-positive and there exists a \\(v\\in \\mathcal{M}\\) such that \\(\\mathcal{F}(v)\\) is finite, then every measure \\(Q\\) has an intrinsic mean. 2.1.2 Geodesics and arc distance Definition 2.1 Let \\(p \\in \\mathcal{M}\\) be arbitray. Consider an open ball \\(B_{\\epsilon &gt;0}\\subseteq T_p\\mathcal{M}\\) of \\(0\\), there exists an unique geodesics \\(\\gamma(t)\\) and an exponential map such that: \\[ \\begin{aligned} \\text{Exp}_{p}: B_{\\epsilon}^n &amp;\\longrightarrow \\mathcal{M} \\\\ \\mathbf{v}&amp;\\longmapsto \\gamma_{v}(t) \\text{ s.t. } \\begin{cases} 0\\in B_{\\epsilon}^n\\subseteq T_p\\mathcal{M}\\\\ \\gamma(0) = p\\\\ \\dot\\gamma(0) = \\mathbf{v}\\\\ \\end{cases}\\\\ \\end{aligned} \\] Definition 2.2 Given any arbitrary \\(x_1,x_2 \\in \\mathcal{M}\\), the Rienmann distance between \\(x_1\\) and \\(x_2\\) is defined by the minimum length of all geodesics connecting them: \\[L = \\inf \\int_{0}^{t}\\gamma(t) dt\\] 2.2 Central Limit Theorem 2.2.1 Sample Estimator for Frechet mean: Definition 2.3 Let \\(\\mathcal{X}_1,\\mathcal{X}_2,\\dots,\\mathcal{X}_n\\) be iid r.v. of the same distribution \\(Q\\) on the same metric space (\\(\\mathcal{M}\\),\\(\\phi\\)). Let the emperical joint distribution be \\(\\hat{Q}_n = \\frac{1}{n}\\sum_{i=1}^n\\partial_{\\mathcal{X}_k}\\), then Frechet sample mean is the Frechet mean of \\(\\hat{Q}_n\\): \\[\\hat{\\Mu}_F = \\{\\hat{p}\\vert\\hat{p} = \\arg\\min_{p}\\frac{1}{n}\\sum_{i=1}^n d^2(\\mathcal{X}_i,p)\\}\\] Theorem 2.1 Let \\(Q\\) be measure on the metric space (\\(\\mathcal{M}\\),\\(\\phi\\)) such that any arbitrary closed and bounded set is also compact in \\(\\mathcal{M}\\), and the event collection \\(\\mathcal{F}\\) is finite: 1) Any arbitrary open ball \\(B_{\\epsilon&gt;0}\\) of Frechet means \\(\\Mu_F\\)contains at least a \\(P\\)-null set \\(N\\) s.t. \\(n(\\delta) &lt; \\infty(\\forall\\delta\\in N^c)\\) whenenver \\(n \\geq n(\\delta)\\). 2) If there exists a Frechet mean of \\(Q\\) then Frechet mean \\(\\hat{\\Mu}_{F}\\) of \\(\\hat{Q}_n\\) is a consistent estimator for \\(\\Mu_F\\) of \\(Q\\) Proof: Let \\(Q\\) be measure with finite \\(\\mathcal{F}\\) on the metric space (\\(\\mathcal{M}\\),\\(\\phi\\)) such that any arbitrary closed and bounded set is also compact in \\(\\mathcal{M}\\). It is possible to pick arbitrary closed and bounded subset \\(\\mathcal{C}\\) under metric space of \\(\\mathcal{M}\\). By definition, \\(\\mathcal{C}\\) is compact. 2.2.2 Sample Estimator for Frechet variance: 2.3 Geodesics Regression X = main_closed(path) qarray = list() for (i in 1:length(X)) { qarray[[i]] = curve_to_q(X[[i]]) } All_mean_shape = find_mean_shape(qarray) saveRDS(All_mean_shape, file = &quot;All_mean_shape.rds&quot;) qmean = All_mean_shape[[1]] qmean_new = project_curve(qmean) pmean = q_to_curve(qmean_new) plot_curve(pmean,&#39;r&#39;,filename = &#39;Mean Shape&#39;) plot_pca_variation(alpha_t_array, qmean, eigdir = 1,qarray, dt = 0) deformation_field_all(alpha_t_array, pmean, qmean, X) 2.4 Isomorphic embedding FPS = function(S, N, first_idx = &#39;a&#39;) { nv = length(S$X) if (first_idx == &#39;a&#39;) { first_idx = randi(nv) } sample = matrix(0, N, 1) D = matrix(0, N, nv) for (i in seq(N)) { if (i == 1) { idx = first_idx } else if (i == 2) { idx= apply(D[1, ],2,max) } else { idx= apply(apply(D[1:i-1, ],2,min),2,max) } sample(i) = idx src = matrix(Inf, nv, 1) src(idx) = 0; D[i,] = fastmarch(S$TRIV, S$X, S$Y, S$Z, src, &#39;single&#39;) } return(List(&#39;D_ext&#39; = D,&#39;sample&#39;=sample)) } initialize_C_sliding_window = function(X1, X2, corr_1, corr_2) { num_pt = length(corr_1) S = num_pt*4 rot_slide = cell(S,1) for (i in seq(S)) { Y1 = X1[corr_1,i:num_pt+i-1]; Y2 = X2[corr_2,i:num_pt+i-1]; temp = svd(t(Y1)%*%Y2,0) U = temp$u V = temp$v rot_slide{i} = U%*%t(V) } rot_curr = matrix(0,S+num_pt-1) rot_W = rot_curr for (i in seq(S)) { rot_W_mat = matrix(0,num_pt); sot_slide_mat = matrix(0,num_pt); if (i == 1) { rot_W_mat[1:end-1, 1:end-1] = 1 sot_slide_mat[1:end-1, 1:end-1] = rot_slide{i}(1:end-1, 1:end-1) } else { rot_W_mat[2:end-1, 2:end-1] = 1 sot_slide_mat[2:end-1, 2:end-1] = rot_slide{i}(2:end-1, 2:end-1) } rot_W[i:i+num_pt-1,i:i+num_pt-1] = rot_W[i:i+num_pt-1,i:i+num_pt-1] + rot_W_mat rot_curr[i:i+num_pt-1,i:i+num_pt-1] = rot_curr[i:i+num_pt-1,i:i+num_pt-1] + sot_slide_mat } rot_initial_final_rec = solve(rot_curr,rot_W) rot_initial_final_rec[rot_curr == 0] = 0 rot_initial_final_rec = rot_initial_final_rec[1:end-1, 1:end-1] temp = svd(rot_initial_final_rec,0) U = temp$u V = temp$v rot_initial_final_rec = U%*%t(V) C = rot_initial_final_rec return(C) } ICP_refine = function(X1, X2, corr_initial, ITER=20, use_ann=FALSE) { corr = corr_initial; for (i in seq(ITER)) { temp = svd(t(X1[corr,])%*%X2,0) U = temp$u V = temp$v C = U%*%t(V) Y1 = X1%*%C if (use_ann) { corr = annquery(t(Y1),t(X2),1) } else { corr = get.knnx(Y1,X2) } } return(list(&#39;corr_refined&#39; = corr,&#39;C&#39; = C)) } FastGeodesics = function(R, sample2){ N = length(sample2); N1 = round(0.5*N); U = rat(sample2) res = eigs((U+t(U))/2,N1,which = &quot;LM&quot;) V = res$values D = res$vectors d = diag(D) S = R d1 = solve(diag(1),d) D1 = diag(d1) I = V%*%D1%*%t(V) return(list(&#39;S&#39; = S,&#39;I&#39; = I)) } GeodesicsBasis = function(S, I){ decomp = qr.solve(S,0) W = decomp$Q %*% I %*% t(decomp$R) res = eigen((W + t(W))/2) Q = decomp$Q %*% res$values beta = res$vectors return(list(&#39;Q&#39; = Q, &#39;beta&#39; = beta)) } shape_match = function(N = 100, rand = 1000, ITER = 5) { nv_1 = length(surface_1$X); nv_2 = length(surface_2$X); corr_true = t(randperm(nv_2,nv_2)) corr_true_reverse[corr_true,1] = seq(nv_2); surface_2$X = surface_2$X[corr_true]; surface_2$Y = surface_2$Y[corr_true]; surface_2$Z = surface_2$Z[corr_true]; surface_2$TRIV = corr_true_reverse(surface_2$TRIV); first_idx = FPS(surface_1, 1) tempr = FPS(surface_1, N, first_idx) D_ext = tempr$D_ext sample_1 = tempr$sample R_1 = t(D_ext) first_idx = FPS(surface_2, 1) tempr = FPS(surface_2, N, first_idx) D_ext = tempr$D_ext sample_2 = tempr$sample R_2 = t(D_ext) tempr = FastGeodesics(R_1, sample_1) S_1 = tempr$S T_1 = tempr$I tempr = FastGeodesics(R_2, sample_2) S_2 = tempr$S T_2 = tempr$I tempr = GeodesicsBasis(S_1, T_1) Q_1 = tempr$S beta_1=tempr$I tempr = GeodesicsBasis(S_2, T_2) Q_2 = tempr$S beta_2=tempr$I X1 = Q_1%*%sqrt(beta_1) X2 = Q_2%*%sqrt(beta_2) landmarks_num = 5; corr_2_5pt = sample_2[1:landmarks_num] corr_1_5pt = corr_true[corr_2_5pt] C_init = initialize_C_sliding_window(X1, X2, corr_1_5pt, corr_2_5pt) sample_rand = randperm(nv_2, rand_num) C_refined = ICP_refine_C(X1, X2[sample_rand,], C_init, ITER, use_ann)$C Y1 = X1%*%C_refined Y2 = X2[sample_rand,] corr_sample_rand = get.knnx(cbind(Re(Y1),Im(Y1)),cbind(Re(Y2),Im(Y2))) perimeter = 0 num_test = pmin(length(sample_rand), 500) err_gdd = matrix(0,num_test,1) len = seq(1,num_test) for (i in len) { idx = corr_true[sample_rand[i]]; src = matrix(0,Inf,1) src(idx) = 0 d = fastmarch(surface_1$TRIV, surface_1$X, surface_1$Y, surface_1$Z, src, &#39;single&#39;) perimeter = apply(perimeter, apply(d,2,max),2,max) err_gdd[i] = d[corr_sample_rand[i]] } err_gdd = err_gdd/perimeter err_sort = sort(err_gdd) corr_full = annquery(t(cbind(Re(Y1),Im(Y1))),t(cbind(Re(X2),Im(X2))), 1) color = d c(surface_1$TRIV, surface_1$X, surface_1$Y, surface_1$Z,color) c(surface_2.TRIV, surface_2$X+60, surface_2$Y+60, surface_2$Z+60,corr_full) } "],["persistence-homology.html", "Chapter 3 Persistence Homology 3.1 Vietoris–Rips complex 3.2 Filtrations 3.3 Reeb Graph inference 3.4 Confidence Set", " Chapter 3 Persistence Homology 3.1 Vietoris–Rips complex The n-simplex (simplicial complex) \\(\\sigma^n \\subset R^{n+1}\\) is defined as: \\[\\sigma^n = \\{(x_0,\\dots,x_n)\\in \\mathbb{R}^{n+1}: x_i \\geq 0 \\text{ } \\forall i; \\sum_{i = 0}^n\\}\\] The idea is we can deconstruct an object (such as a network) into more simple form (such as point and edges). Here, a 0-simplex is a point and the 1-simplex is a segment and so on. Vietoris–Rips complex is the abstract simplicial complex whose k-simplices correspond to subsets of points in P with diameter at most \\(2t\\), \\[\\mathcal{R}(P,t) = \\{ \\sigma |\\emptyset \\neq \\sigma \\subset P, Diam(\\sigma) \\leq 2t\\} \\] ## Persistent Diagram At each intervals \\((d_1,d_2)\\), we record the homology at radius \\(r_t\\) through time. At the end, we can have the Persistent diagram uniquely identified the point cloud. Persistent homology is robust against noise, where we can quantify our metric distance by the bottleneck distance: \\[W_{\\infty}(X,Y) = \\inf_{\\Gamma: X\\to Y} \\sup_{x \\in X} \\|x -\\Gamma(x) \\|_{\\infty}\\] We then can compare the method with other cluster algorithm such as principle component analysis, support vector machine or hierarchical clustering. 3.2 Filtrations Point cloud \\(\\mathcal{X}\\) is simply the representation of data in space, then by replacing \\(\\mathcal{X}\\) with a nested interval Vietoris-Rips complexes. By taking the homology of each in the filtration of simplicial complexes, we can have obtain the persistent module as our features vectors for computation: \\[H_i(\\mathcal{X}_{r_0}) \\to H_i(\\mathcal{X}_{r_1}) \\to H_i(\\mathcal{X}_{r_2}) \\to \\dots\\] The input of our problem is the gene expression matrix which can be represented as an object in \\(\\mathbb{R}^3\\) spaces: the expression value, the sample index and the gene name. The Mapper utilizes this property to preserve the local relationships of high dimensional expression data while clustering. 3.3 Reeb Graph inference For constant threshold \\(\\epsilon = 10^{-6}\\), let the metric space \\((X,\\partial_X)\\) where \\(X\\) is the gene expression matrix and \\(\\partial_X: X\\times X \\to \\mathbb{R}\\) defined by the Pearson’s correlation: \\[\\partial_X = \\frac{\\\\Cov(X,X)}{\\sigma_X^2}\\] Let the filter function \\(f: X \\to \\mathbb{R}^{n}\\) finds the first 2 nearest neighbors of each element in \\(X\\) in the Euclidean norm \\(\\mathcal{L}^2\\), then \\(\\mathcal{C} = \\{U_{\\alpha}\\}\\) is a cover of the range of \\(f\\) in \\(\\mathbb{R}\\). Define the equivalence relation, \\(\\sim\\), such that \\(a \\sim b\\) if \\(a,b \\in f^{-1}(U_{\\alpha})\\) for arbitrary \\(\\alpha\\). The Reeb space of X is the quotient of X by this equivalence relation. For appropriate choices of interval and percentages overlaps, we can have: 3.4 Confidence Set "],["application.html", "Chapter 4 Application 4.1 Gene Expression 4.2 Generative model for gene co-expression network 4.3 Data Collection and Pre-processing 4.4 Network Visualization 4.5 Heavy tailed distribution analysis 4.6 Betti Number Inference", " Chapter 4 Application 4.1 Gene Expression The task of classifying the cancer regulatory pathways of patients becomes important, and mathematics have provided us the tools to do so. The gene expression is one of many valuable biometric information to identify cancer cells, in particular, the mRNA copies presented in tumors. From this data, one can construct the gene co-expression network in order to analyze the behavior of active cancer genes. This capstone focuses on analyzing the difference of topology throughout AML networks in order to see if one can predict or dentify the biomark of AML to improve the prognosis of the disease or discover effective drug pathways. 4.2 Generative model for gene co-expression network Define the gene co-expression network that we want as \\(G = (\\mathcal{V}, \\mathcal{E})\\), where the vertex set \\(\\mathcal{V}\\) consists of genes. An edge in the edge set \\(\\mathcal{E}\\) denotes if there is similarity in expression level between 2 genes, which results in G being an undirected network. For constant threshold \\(\\epsilon = 10^{-6}\\), let the metric space \\((X,\\partial_X)\\) where \\(X\\) is the gene expression matrix and \\(\\partial_X: X\\times X \\to \\mathbb{R}\\) defined by the Pearson’s correlation: \\[\\partial_X = \\frac{\\\\Cov(X,X)}{\\sigma_X^2}\\] Let the filter function \\(f: X \\to \\mathbb{R}^{n}\\) finds the first 2 nearest neighbors of each element in \\(X\\) in the Euclidean norm \\(\\mathcal{L}^2\\), then \\(\\mathcal{C} = \\{U_{\\alpha}\\}\\) is a cover of the range of \\(f\\) in \\(\\mathbb{R}\\). Define the equivalence relation, \\(\\sim\\), such that \\(a \\sim b\\) if \\(a,b \\in f^{-1}(U_{\\alpha})\\) for arbitrary \\(\\alpha\\). The Reeb space of X is the quotient of X by this equivalence relation. For appropriate choices of interval and percentages overlaps, we can have: Initialized empty graph G = \\((\\mathcal{V},\\mathcal{E})\\); Initialized parameters \\(\\partial_X\\), \\(f\\), \\(\\mathcal{C}\\) as stated; discretization of Reeb space of X : Cluster each inverse image \\(f^{-1}(U_{\\alpha})\\subseteq X\\) \\(\\forall U_{\\alpha} \\in \\mathcal{C}_{\\alpha,i}\\); Add elements in \\(\\mathcal{C}_{\\alpha,i}\\) to \\(\\mathcal{V}\\); If \\(\\mathcal{C}_{\\alpha,i} \\cap\\mathcal{C}_{\\alpha^{\\prime},i^{\\prime}} = \\emptyset\\) then add \\(e_{\\mathcal{C}_{\\alpha,i},\\mathcal{C}_{\\alpha^{\\prime},i^{\\prime}}}\\) to \\(\\mathcal{E}\\); Color \\(v \\in \\mathcal{C}_{\\alpha,i} \\subset \\mathcal{V}\\) by average value of \\(f\\) on \\(v\\). G = \\((\\mathcal{V},\\mathcal{E})\\) 4.3 Data Collection and Pre-processing I used the gene expression dataset from The Cancer Genome Atlas (TCGA) for both the training set (TCGA-LAML) and the validation set (query sample from healthy bone marrow and blood tissues). The data is obtained experimentally through Illumina HiSeq 2000 RNA Sequencing, which are \\(log_2(x+1)\\)-RSME normalized with 95% credibility interval. Heat map: 4.4 Network Visualization 4.5 Heavy tailed distribution analysis 4.6 Betti Number Inference "],["random.html", "Chapter 5 random", " Chapter 5 random "],["code-need-to-fix.html", "Chapter 6 code need to fix", " Chapter 6 code need to fix read_svg_file2 &lt;- function(file_path, N = 100, r = 10) { #Function require XML library. Read in the command in the SVG file file_type = &quot;O&quot; doc &lt;- XML::htmlParse(file_path) p &lt;- XML::xpathSApply(doc, &quot;//path&quot;, XML::xmlGetAttr, &quot;d&quot;) if (stringr::str_detect(p, &quot;z&quot;)) { file_type = &quot;C&quot; } if (file_type == &quot;C&quot;) { element &lt;- strsplit(p, &quot;\\\\, |\\\\,| &quot;)[[1]] #Create the starting point and make sure we create the starting point correctly segment = c() curr_pos = c(0,0) pos = c(as.numeric(element[2]),as.numeric(element[3])) curr_pos = curr_pos + pos start_pos = curr_pos control1 = c(as.numeric(element[5]), as.numeric(element[6])) control2 = c(as.numeric(element[7]), as.numeric(element[8])) end = c(as.numeric(element[9]), as.numeric(element[10])) control1 = control1+curr_pos control2 = control2+curr_pos end = end + curr_pos segment_part = matrix(c(start_pos[1],control1[1], control2[1],end[1], start_pos[2],control1[2], control2[2],end[2]), 4,2) segment = list() segment[[1]] = segment_part curr_pos = end j=2 for (i in seq(from = 11, to = length(element)-1, by = 6)) { start_pos = curr_pos control1 = c(as.numeric(element[i]), as.numeric(element[i+1])) control2 = c(as.numeric(element[i+2]), as.numeric(element[i+3])) end = c(as.numeric(element[i+4]), as.numeric(element[i+5])) control1 = control1+curr_pos control2 = control2+curr_pos end = end + curr_pos curr_pos = end segment_part = matrix(c(start_pos[1],control1[1], control2[1],end[1],start_pos[2], control1[2],control2[2], end[2]), 4,2) segment[[j]] = segment_part j = j+1 } point &lt;- c() for (i in 1:length(segment)) { xy &lt;- knotR::bezier(segment[[i]], n = r) point &lt;- rbind(point, xy) } point_resample &lt;- resample_curve(t(point), N = N) return(point_resample) } else { p_modified &lt;- stringr::str_extract_all(p,&quot;\\\\-?[0-9.cC]+&quot;)[[1]] element &lt;- unlist(strsplit(p_modified, &quot;(?=[A-Za-z])(?&lt;=[0-9])|(?=[0-9])(?&lt;=[A-Za-z])&quot;, perl=TRUE)) segment = c() curr_pos = c(0,0) pos = c(as.numeric(element[1]),as.numeric(element[2])) curr_pos = curr_pos + pos start_pos = curr_pos control1 = c(as.numeric(element[4]), as.numeric(element[5])) control2 = c(as.numeric(element[6]), as.numeric(element[7])) end = c(as.numeric(element[8]), as.numeric(element[9])) control1 = control1+curr_pos control2 = control2+curr_pos end = end + curr_pos segment_part = matrix(c(start_pos[1],control1[1], control2[1],end[1],start_pos[2], control1[2],control2[2],end[2]), 4,2) segment = list() segment[[1]] = segment_part curr_pos = end j=2 for (i in seq(from = 9, to = length(element), by = 7)) { start_pos = curr_pos control1 = c(as.numeric(element[i+1]), as.numeric(element[i+2])) control2 = c(as.numeric(element[i+3]), as.numeric(element[i+4])) end = c(as.numeric(element[i+5]), as.numeric(element[i+6])) if (element[i] != &quot;C&quot;) { control1 = control1+curr_pos control2 = control2+curr_pos end = end + curr_pos } curr_pos = end segment_part = matrix(c(start_pos[1],control1[1], control2[1],end[1],start_pos[2], control1[2],control2[2],end[2]), 4,2) segment[[j]] = segment_part j = j+1 } point &lt;- c() for(i in 1:length(segment)){ xy &lt;- knotR::bezier(segment[[i]], n=4) point &lt;- rbind(point, xy) } point_resample &lt;- resample_curve(t(point), N = N) return(point_resample) } } num_spline = 2 path = NULL #path = paste(&quot;/Users/macalesteritsloaner/Downloads/bookdown-demo-master/dataset/spline/no.svg&quot;,1:num_spline,&quot;.html&quot;, sep = &quot;&quot;) #write.csv(path, file = &quot;/Users/macalesteritsloaner/Downloads/bookdown-demo-master/dataset/spline/pathinfo.csv&quot;, row.names = FALSE) #path = &quot;/Users/macalesteritsloaner/Downloads/bookdown-demo-master/dataset/spline/pathinfo.csv&quot; verify_sample_shapes &lt;- function(pathname){ fid = read.table(pathname,stringsAsFactors = FALSE)[[1]] for (i in 2:length(fid)) { fname = fid[i] X = read_svg_file2(fname) X = t(X) print(X) plot_curve(X,&#39;r&#39;, l = FALSE, filename = fname) } } #verify_sample_shapes(path) fastmarch = function(TRIV, X, Y, Z, src, opt) { mode = 1 if ((mode &amp; length(src)) != prod(dim(X))) { error(&#39;src must be nvx1 in the single source mode.&#39;) } idx = which(src==0) srclabel = matrix(0,length(src),1) srclabel[idx] = seq(length(idx)) temp = fastmarch1_mex(int32(TRIV-1), double(rbind(prod(dim(src)), prod(dim(srclabel))), double(prod(dim(X))), double(prod(dim(Y))), double(prod(dim(Z))) ) ) D = temp$D L = temp$L D[D&gt;=9999999] = Inf return(list(&#39;D&#39;=D,&#39;L&#39;=L)) } "],["references.html", "References", " References "]]
