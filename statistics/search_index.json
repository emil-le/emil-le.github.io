[["preface.html", "Statistical methods on Riemannian manifold with applications. Preface", " Statistical methods on Riemannian manifold with applications. Emil Le 2021-03-14 Preface Statistical inference has multiple applications in the field of computer vision, which dealt with interdisciplinary topics ranging from biomedical imaging, object recognition, motion tracking and so on. Often time, random variable on the real domain cannot capsule the geometric complexity of the event space we are trying to model, for example, flows, luminosity, contrast, etc, which are sometime one of the most important features of the data. This calls for development on statistical methodologies on different domain space. In this capstone, we try to introduce the relevancy of introductory concepts and applications of statistics on the Riemannian manifold. Figure: Isomap segmentation of the Swiss Roll "],["intro.html", "Chapter 1 Introduction 1.1 Motivation 1.2 Project outline 1.3 What is a Manifold?", " Chapter 1 Introduction 1.1 Motivation Before getting into definition of what a manifold is, we can intuitively think of it as a space that locally resembles the Eucledian space. Manifold learning, in simplest term, is a collection of methods that represents data on a manifold domain. As we progress into the topics in this capstone, we can see that many methodologies and algorithms are built on the same intuition as of those in Euclidean spaces. In general, manifold learning assumes the manifold hypothesis, which states that high-dimensional data lies on a low-dimensional manifold embedded in a higher-dimensional space. The interest in manifold methods raise from the fact that it provides a better interpolation for data that has geometric features, which linear methods might fail to incorporate in. These are very abstract ideas to think about without examples, so this capstone project will try to explain some of the aformentioned ideas in the respective topics. 1.2 Project outline The main objective of this capstone is trying to prove some of the fundamental theorem in statistics on an Riemannian manifold domain. Though for every section, there will be motivation and analogy on why some of the concepts are developed and an application to illustrate these ideas. The 3 topics being explored are How are some fundamental statistical theory defined on an Riemannian manifold domain? This is explored in section 2. Geodesics regression is explored on section 3. My contribution to this project includes trying to replicate the methodologies and prove statistics theorem with property of the Riemannian manifold. 1.3 What is a Manifold? To be more specific, the manifold in this project refers to the one defined by topological axioms. As such, some of the proofs in this project might be a bit strange for proofs done in statistics that we have seen but the intuition is very analogous. Informally, we can think of a manifold as an abstract shape in space that locally resembles to the Euclidean space. For example, the Earth is an 2-manifold because we perceive the local environment on earth as a flat surface. The formal definition to quantify the notion of “similarity” in shape is called “homeomorphism.” Definiton Homeomorphism For arbitrary topological space \\(\\mathcal{X},\\mathcal{Y}\\) if there exists \\(f: \\mathcal{X}\\to\\mathcal{Y}\\) such that \\(f\\) is a continuous bijection then (1) \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) are homeomorphic (denote as \\(\\mathcal{X}\\cong\\mathcal{Y}\\)) (2) \\(f\\) is a homeomorphism. Example A hollow 3D unit sphere (denoted as \\(S^2\\)) minus the north point is homeomorphic to the plane: \\(S^2\\setminus\\{(0,0)\\}\\cong \\mathbb{R}^2\\). A nice way to think about this is if we image poking a needle to a baloon, it would collapse to a flat surface (as shown in the figure below). \\[\\text{Figure 1: &quot;Poking&quot; the north point of the sphere resulted in a plane.}\\] Definiton An \\(n-\\)manifold \\(\\mathcal{M}\\) (without bounfary) is a \\(T_2\\) topological space (Hausdorff and with countable basis), where for every \\(m\\in\\mathcal{M}\\) there exists a neighborhood \\(U\\subseteq \\mathcal{M}\\) such that \\(U\\) is homeomorphic to an open ball in space \\(\\mathbb{R}^n\\) induced by the Eucledian metric. Example If we were to draw a circle with an infinitely small radius on the surface of \\(S^2\\), it would look like a circle in the plane. A torus is also a manifold by the same resonance (our earth could have been a donut!!). In general, a manifold without specification refers to manifold without boundary/ closed (so it must be a compact space). A manifold with boundary is not a manifold, for example, the Mobius is not a manifold. \\[\\text{Figure 2: The torus is a manifold but the mobius strip is not.}\\] The majority of this project only focuses on the Rienmannian manifold because its properties allow us to apply results from calculus (or at least an analogous way) to study distribution on manifold. "],["statistics-on-riemannian-manifold.html", "Chapter 2 Statistics on Riemannian Manifold 2.1 Riemannian Manifold 2.2 Instrinsic moments 2.3 Asymtomtic behavior 2.4 Normal distribution", " Chapter 2 Statistics on Riemannian Manifold 2.1 Riemannian Manifold An important property that an \\(n-\\)manifold \\(\\mathcal{M}\\) has is that it is compact (by defninition) and is metricizable (Urysohn’s theorem). In this section, we want to construct a metric for an abstract manifold whose properties allow us to do calculus. Recall that the tangent space is \\(\\mathcal{M}\\) is homeomorphic to \\(\\mathbb{R}^n\\), we define Riemannian metric as one that maps a vector in the tangent space to its norm (essentially quantify the vector): Definition Let \\(T_p\\mathcal{M}\\) be the tangent space of a point \\(p\\) in an differentiable manifold \\(\\mathcal{M}\\subset \\mathbb{R}^n\\). The Riemannian manifold is defined as the topological space on \\(\\mathcal{M}\\) induced by the Riemannian metric \\(\\rho\\) by: \\[ \\begin{aligned} \\langle ., .\\rangle_p: T_p\\mathcal{M}\\times T_p\\mathcal{M} &amp;\\rightarrow \\mathbb{R}\\\\ \\vec{\\mathbf{v}} &amp;\\longmapsto \\sqrt{\\langle \\vec{\\mathbf{v}},\\vec{\\mathbf{v}} \\rangle_p}\\\\ \\end{aligned} \\] Recall that in a plane, there many way that we can go from one point to another, but the shortest path is a straight line. We want to construct similar idea on the manifold is defined through curve and geodesics. A curve on a manifold is a path that connects 2 arbitrary point on \\(\\mathcal{M}\\) (i.e. how can we go from one point to another). The geodesics between 2 arbitrary point on a manifold is simply the shortest curve containing those 2. Definition A curve in \\(\\mathcal{M}\\) is defined as \\(\\gamma: [0,1]\\to \\mathcal{M}\\) such that \\(\\gamma\\) is differentiable. The length of a curve is defined as \\[L(\\gamma) = \\int_0^1 \\sqrt{\\langle \\gamma(t)^{&#39;} \\rangle_{\\gamma(t)}} dt\\] Definition The geodesics between arbitrary \\(x,y\\in\\mathcal{M}\\) is given by the metric \\(\\rho: \\mathcal{M}\\times \\mathcal{M} \\to \\mathbb{R}\\): \\[ \\rho(x,y) = \\inf\\{L(\\gamma) \\vert \\gamma(0) = x, \\gamma(1) = y)\\} \\] Equipped together, (\\(\\mathcal{M},\\rho\\)) is a Rienmannian manifold. Figure below visualize a tangent vector at the point Saint Paul and a geodesics between Saint Paul and Ho Chi Minh City on a sphere. Figure 2.1: The geodesics between Saint Paul and Ho Chi Minh City on Earth! 2.2 Instrinsic moments In order to give rigorous definition about probability in a manifold domain, there are quite several concepts in measure theory we need to be aware of. In general, a probability space \\((\\Omega,\\mathcal{F},\\mathcal{P})\\) consists of 3 elements: Sample space \\(\\Omega\\): the set of all possible outcome. Event space \\(\\mathcal{F}\\): the collection of events, each event is a set of outcome in the sample space Probability measure \\(\\mathcal{P}:\\mathcal{F}\\to[0,1]\\) a function define the probability of events (which have satisfy a few axioms) Since this is quite out of scope to this project (which can be read more about in this link), we can simply think of the point of defining probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) is to “do” statistics in usual probability theory we have seen in class. For the purpose of this project, we assume the probability space are complete, a random variable would take places in \\((\\mathcal{M},\\mathcal{B})\\) where \\(\\mathcal{M}\\) is complete Rienmannian manifold \\(\\mathcal{B}\\) is the Borel \\(\\sigma-\\)algebra generated by the topology induced by Rienmannian metric \\(\\rho\\). For an abstract manifold, there are many ways we can define a “mean” such as intrinsic mean and extrinsic mean. Though for a Riemannian manifold with geodesic metric, we only focus on the intrinsic mean and variance. Definition Fréchet moments Let \\((\\mathcal{M},\\rho)\\) be abstract Riemannian manifold space with measure \\(\\mu\\). The Fréchet mean and variance are defined respectively as: \\[\\Theta^2 =\\arg\\inf_{p\\in\\mathcal{M}}\\int_\\mathcal{M} \\rho(x,p)^2 d\\mu(x)\\] \\[\\sigma^2 =\\inf_{p\\in\\mathcal{M}}\\int_\\mathcal{M} \\rho(x,p)^2 d\\mu(x)\\] It is worth to point out that \\(\\Theta^2\\) is a collection of Fréchet mean as the Fréchet mean is not guaranteed or unique in an abstract manifold. Although since the Riemannian manifold is a Hilbert space, there exists a unique global argument of the infimum so \\(\\Theta^2\\) is a singleton.Here, if we replace the domain \\(\\mathcal{M}\\)with \\(\\mathbb{R}^n\\), then it is our usual understanding of expectation! Definition: Sample Fréchet mean and variance Let there be finite collection of \\(n\\) random variables \\(\\mathcal{X}_i: \\Omega\\to\\mathcal{M}\\), the sample Fréchet mean and variance is defined as: \\[\\hat{\\Theta}^2 = \\arg\\min_{p\\in\\mathcal{M}} \\frac{1}{n}\\sum_{i=1}^n \\rho^2(\\mathcal{X}_i,p)\\] \\[\\hat\\sigma^2 = \\min_{p\\in\\mathcal{M}} \\frac{1}{n}\\sum_{i=1}^n \\rho^2(\\mathcal{X}_i,p)\\] By changing the geodesics metric with Eucledian metric for elements in \\(\\mathbb{R}^2\\), we simply arrive at the sample mean and variance we typically seen! We need to prove that \\(\\hat\\mu_F\\) and \\(\\hat\\sigma_F\\) exist for \\((\\mathcal{M},\\rho)\\) that is complete. Proof Let \\(X = \\{x_1,x_2,\\dots,x_n\\}\\subset\\mathcal{M}\\) be a finite collection of \\(n\\) points on a complete Riemannian space \\((\\mathcal{M},\\rho)\\). Let \\(\\mathcal{f}: \\mathcal{M} \\to \\mathbb{R}\\) be given by \\[\\mathcal{f}(m) = \\sum_{i=1}^n \\rho^2(m,x_i)\\] It is sufficient to show that \\(\\mathcal{f}(m)\\) achieves a minimum. Let \\(\\epsilon = \\max\\{\\rho(x_i,x_j)\\vert i\\neq j\\}\\) be the maximum distance, consider the finite union of closed \\(\\rho-\\)ball cenetered at \\(x_i\\) \\(\\bar{B}_\\rho(\\epsilon,x_i)\\) for all \\(x_i\\in X\\): \\[C = \\bigcup_{i=1}^n \\bar{B}_\\rho(\\epsilon,x_i)\\] It follows that \\(C\\) is a closed and bounded subset of the complete metric space \\(\\mathcal{M}\\), which implies \\(C\\) is compact. For arbitrary \\(x\\in C\\) and \\(\\varepsilon &gt;0\\) arbitrary, let there be an open Eucledian ball \\(B_d(\\varepsilon,\\mathcal{f}(x))\\) of \\(\\mathcal{f}(x)\\). Then \\(\\mathcal{f}^{-1}(B_d(\\varepsilon,\\mathcal{f}(x))\\) is an neighborhood of \\(x\\) in \\(\\mathcal{M}\\), pick \\(\\delta &gt; 0\\) such that \\(\\delta &lt; \\epsilon\\). For arbitrary \\(m&#39;\\in \\mathcal{f}^{-1}(B_d(\\varepsilon,\\mathcal{f}(x))\\), the open ball \\(B_\\rho(\\delta,m&#39;)\\subset \\mathcal{f}^{-1}(B_d(\\varepsilon,\\mathcal{f}(x)))\\) so pre-image of open Eucledian ball is open in \\(\\mathcal{M}\\). Thus, \\(f\\) is continuous, by Extreme Value Theorem, the \\(\\mathcal{f}\\) has a global minimum. Figure 2.2: The sample Frechet mean on a \\(2-\\)sphere. 2.3 Asymtomtic behavior Theorem Almost surely convergence of Fréchet mean and variance For probability space \\((\\Omega,\\mathcal{F},\\mathcal{P})\\) and complete Rimennian metric space \\((\\mathcal{M},\\rho)\\), suppose there are finite sequence of \\(n\\) independent and identically distributed random variable (iid r.v.) \\(\\mathcal{X}_i:\\Omega\\to\\mathcal{M}\\) for \\(i=1,2,\\dots,n\\). It follows that: \\[ \\begin{aligned} &amp;\\lim\\sup\\hat\\Theta^2\\subseteq\\Theta^2 \\text{ }\\text{ }\\text{ }\\text{ }\\text{ }\\text{ }\\text{ and }\\text{ }\\text{ }\\text{ }\\text{ }\\text{ }\\text{ } \\hat\\sigma^2_n \\to \\sigma^2 \\end{aligned} \\] Proof Let there be complete Rimennian metric space \\((\\mathcal{M},\\rho)\\) and probability space \\((\\Omega,\\mathcal{F},\\mathcal{P})\\) and measure \\(\\mu\\). For arbitrary \\(n\\) i.i.d r.v. \\(\\{\\mathcal{X}_i\\}_{i=1,2,\\dots, n}\\). Define the sequence of function \\(F\\) and \\(F^\\ast\\): \\[ \\begin{aligned} &amp;F_n(z) = \\frac{1}{n}\\sum_{i=1}^n \\rho^2(z,\\mathcal{X_i}) - \\int_\\mathcal{M}\\rho^2(z,x)d\\mu(x) \\\\ &amp;F^\\ast_n(z) = \\frac{1}{n}\\sum_{i=1}^n \\rho^2(z,\\mathcal{X_i}) - \\int_\\mathcal{M}\\rho^2(\\Theta,x)d\\mu(x) \\\\ \\end{aligned} \\] Since \\(F_n(z)\\) is a random variable on the real domain, by Strong Law of Large number, for arbitrary \\(z\\in\\mathcal{M}\\) it follows that: \\[\\lim_{n\\to\\infty} F_n(z) =0\\] Since \\((F_n)_{n\\in\\mathbb{N}}\\) uniformly converges, by compactness of \\(\\mathcal{M}\\): \\[\\lim_{n\\to\\infty}\\sup_{z&#39;\\in\\mathcal{M}} F_n(z&#39;) =0 \\implies\\lim_{n\\to\\infty}F_n(\\hat\\Theta_n) =0\\] Since \\(\\Theta\\) is the greatest upper bound and \\(\\hat\\Theta_n\\) (\\(\\forall n\\in\\mathbb{N}\\)) is the global minimum of the domain, it follows that \\(\\hat\\Theta_n\\leq \\Theta\\) (\\(\\forall n\\in\\mathbb{N}\\)). Then: \\[ \\begin{aligned} F_n(\\hat\\Theta_n) &amp;= \\frac{1}{n}\\sum_{i=1}^n \\rho^2(\\hat\\Theta_n,\\mathcal{X_i}) - \\int_\\mathcal{M}\\rho^2(\\hat\\Theta_n,x)d\\mu(x) \\\\ &amp;\\leq \\frac{1}{n}\\sum_{i=1}^n \\rho^2(\\hat\\Theta_n,\\mathcal{X_i}) - \\int_\\mathcal{M}\\rho^2(\\Theta,x)d\\mu(x) \\\\ &amp;= F^\\ast_n(\\hat\\Theta_n)\\\\ F^\\ast_n(\\hat\\Theta_n) &amp;= \\frac{1}{n}\\sum_{i=1}^n \\rho^2(\\hat\\Theta_n,\\mathcal{X_i}) - \\int_\\mathcal{M}\\rho^2(\\Theta,x)d\\mu(x) \\\\ &amp;\\leq \\frac{1}{n}\\sum_{i=1}^n \\rho^2(\\Theta,\\mathcal{X_i}) - \\int_\\mathcal{M}\\rho^2(\\Theta,x)d\\mu(x) \\\\ &amp;= F_n(\\Theta) \\end{aligned} \\] \\[\\implies F_n(\\hat\\Theta_n)\\leq F^\\ast_n(\\hat\\Theta_n) \\leq F_n(\\Theta)\\] Define \\(K_n = \\max\\{|F_n(\\hat\\Theta_n)|,|F_n(\\Theta)|\\}\\) then: \\[ \\begin{cases} \\lim_{n\\to\\infty}K_n = 0\\\\ |F^\\ast_n(\\hat\\Theta_n)| \\leq K_n \\end{cases} \\implies \\lim_{n\\to\\infty}|F^\\ast_n(\\hat\\Theta_n)| =0 \\implies \\hat \\sigma^2\\to\\sigma^2 \\] Thus, the Fréchet variance is almost surely convergence (a.s.), and it’s left to prove the Fréchet expectation is also a.s.. For arbitrary \\(n\\in\\mathbb{N}\\), define the subset \\(\\mathcal{C}_n\\subset\\mathcal{M}\\) as \\[\\mathcal{C}_n = \\text{Closure}\\left(\\bigcup_{m=n}^\\infty \\hat\\Theta^2_m\\right)\\] We will prove \\(C_n = \\hat\\Theta^2_n\\) by inclusion both way. Let \\(n\\in\\mathbb{N}\\) and \\(c_j\\in\\mathcal{C}_n\\) be arbitrary. Since \\(\\mathcal{C}_n\\) is closed, by compactness of \\(\\mathcal{M}\\), there exists a subsequence \\((c_{j_k})\\subset \\mathcal{C}_{n_k}\\) such that \\(c_{j_k} \\to c_j\\). Then \\(c_j\\in\\lim_{n\\to\\infty}\\sup_{j\\geq n} \\mathcal{C}_n \\implies c_j\\in \\lim\\sup\\hat\\Theta^2_n\\), which implies \\(C_n\\subseteq\\lim\\sup\\hat\\Theta^2_n\\). Now for the converse, let \\(c_j\\in\\lim\\sup\\hat\\Theta_n\\) be arbitrary. By definition, for every \\(\\epsilon&gt;0\\), there exists subsequence \\((c_{j_k})\\subset\\mathcal{C}_{n_k}\\) satisfying \\(c_{j_k}\\to c_j\\). Then \\(c_j\\in\\text{Closure}(\\bigcup_{m=n}^\\infty\\hat\\Theta^2_m)\\), so \\(\\lim\\sup\\hat\\Theta^2_n\\subseteq\\mathcal{C}_n\\). By inclusion both way, \\(\\lim\\sup\\hat\\Theta^2_n=\\text{Closure}\\left(\\bigcup_{m=n}^\\infty \\hat\\Theta^2_m\\right)\\) Let \\(\\hat\\theta\\in\\lim\\sup\\hat\\Theta^2_n\\) be arbitrary then \\(\\hat\\theta\\in\\text{Closure}\\left(\\bigcup_{m=n}^\\infty \\hat\\Theta^2_m\\right)\\). By compactness of \\(\\mathcal{M}\\), there exists a convergent subsequence \\((\\hat\\theta_k)\\) such that \\(\\hat\\theta_k\\to\\hat\\theta\\). Thus, we can make \\(\\rho^2(\\hat\\theta_k,\\hat\\theta)\\) as mall as we want. Pick \\(N_k \\in \\mathbb{N}\\) such that \\(N_k\\geq \\frac{1}{\\varepsilon}\\) for \\(\\varepsilon&gt;0\\), so whenever \\(k\\geq N_k\\) it must be that \\(\\rho^2(\\hat\\theta_k,\\hat\\theta) \\leq \\frac{1}{k}\\). By Minkowski inequality in \\(L^2\\) space, we have: \\[ \\begin{aligned} \\left(\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\rho^2(\\mathcal{X}_i,\\hat\\theta)\\right)^{1/2} &amp;\\leq \\left(\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\rho^2(\\mathcal{X}_i,\\hat\\theta_k)\\right)^{1/2} + \\left(\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\rho^2(\\hat\\theta_k,\\hat\\theta)\\right)^{1/2}\\\\ &amp;\\leq \\left(\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\rho^2(\\mathcal{X}_i,\\hat\\theta_k)\\right)^{1/2} + \\left(\\frac{1}{N_k}N_k(\\frac{1}{k})^2\\right)^{1/2}\\\\ &amp;\\leq \\left(\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\rho^2(\\mathcal{X}_i,\\hat\\theta_k)\\right)^{1/2} + \\frac{1}{k}\\\\ \\implies \\lim\\inf_{k\\to\\infty}\\left(\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\rho^2(\\mathcal{X}_i,\\hat\\theta)\\right)^{1/2} &amp;\\leq \\lim\\inf_{k\\to\\infty}\\left(\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\rho^2(\\mathcal{X}_i,\\hat\\theta_k)\\right)^{1/2} + \\lim\\inf_{k\\to\\infty}\\frac{1}{k} \\\\ \\implies \\mathbb{E}\\left[\\rho^2(\\mathcal{M},\\hat\\theta)\\right]^{1/2} &amp;\\leq \\lim\\inf_{k\\to\\infty}\\left(\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\rho^2(\\mathcal{X}_i,\\hat\\theta_k)\\right)^{1/2} + 0\\\\ &amp;\\leq \\lim\\inf_{k\\to\\infty}\\left(\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\rho^2(\\mathcal{X}_i,z&#39;)\\right)^{1/2} (\\forall z&#39;\\in\\mathcal{M})\\\\ &amp;\\leq \\mathbb{E}\\left[\\rho^2(\\mathcal{M},z&#39;)\\right]^{1/2} \\\\ \\implies \\mathbb{E}\\left[\\rho^2(\\mathcal{M},\\hat\\theta)\\right] &amp;\\leq \\mathbb{E}\\left[\\rho^2(\\mathcal{M},z&#39;)\\right]\\\\ \\implies \\hat\\theta \\in \\Theta^2 &amp;\\implies \\lim\\sup\\hat\\Theta_n\\subseteq\\hat\\Theta \\end{aligned} \\] Thus, the sample Fretchet mean and variance is Almost surely converges. 2.4 Normal distribution There is not a general Normal distribution defined on an abstract manifold, so the following definition of normal density function only applies to the Riemannian manifold. Definition For a Riemannian manifold \\((\\mathcal{M},\\rho)\\), the probability density function for normal distribution is \\(\\mathcal{Pr}:(\\mathcal{M},\\mu,\\tau)\\to[0,1]\\) defined by: \\[ \\begin{aligned} \\mathcal{Pr}(x\\vert\\mu,\\tau) &amp;= \\frac{1}{C(\\mu,\\tau)}e^{-\\frac{\\tau}{2}\\rho(\\mu,x)^2} \\\\ \\text{Where :}&amp;\\\\ C(\\mu,\\tau) &amp;= \\int_\\mathcal{M}e^{-\\frac{\\tau}{2}\\rho(\\mu,x)^2}dx \\\\ &amp;\\text{ is a normalized term.} \\end{aligned} \\] A random variable \\(\\mathcal{X}\\) of \\((\\mathcal{M},\\rho)\\) follows a Riemannian normal distribution is denoted as \\(\\mathcal{X}\\sim\\text{N}_\\mathcal{M}(\\mu,\\tau^{-1})\\). The figures below shows the sampling on \\(\\text{N}_\\mathcal{M}(0,1)\\), respectively, at \\(n=30,100,1000\\). We can see that it shares some similar symmetrical dispersion property pn the domain as sampling normal distribution on \\(\\mathbb{R}\\). Another interesting result is that the central limit theorem on Riemannian also holds! Figure 2.3: Sampling from Normal distribution on the \\(2-\\)sphere. Definition Central limit theorem Let \\(\\mathcal{Y}_1,\\mathcal{Y}_2,\\mathcal{Y}_3,\\dots,\\mathcal{Y}_n\\) be \\(n\\) independent random variable from the same distribution (same parameters) then \\[\\sqrt{n} (\\mathcal{Y}_1+\\mathcal{Y}_2+\\dots+\\mathcal{Y}_n)\\longrightarrow_\\rho\\text{N}_\\mathcal{M}(\\vec{0},\\tau^{-1})\\] The proof of this theorem deserves a project of its own! Look’s at how interesting the statistical theories on the Riemannian manifold intutively similar to the \\(\\mathbb{R}^n\\)! "],["geodesics-regression.html", "Chapter 3 Geodesics Regression 3.1 Motivation 3.2 Geodesics Regression 3.3 Least Square Estimate 3.4 Application", " Chapter 3 Geodesics Regression 3.1 Motivation Before we get into geodesics regression, let’s recall a simple fitting model that reflects similar idea: linear gression. Let \\(\\mathbf{X} \\in \\mathbb{R}^{n\\times m}\\) be a \\(n\\times m\\) matrix consists of our data points that we want to model against \\(\\mathbf{y}\\in\\mathbb{R}^{n}\\). The question we ask here is how to find an \\(m^{\\text{th}}-\\)degree polynomial that is best-fit to our data (for example, if \\(m=1\\) then it’s called line of best fit). The linear regression model is given as: \\[ \\begin{aligned} \\mathbf{y}_i &amp;= \\mathbf{x}_{i1}^T\\beta_1 + \\mathbf{x}_{i2}^T\\beta_2 + \\dots + \\mathbf{x}_{in}^T\\beta_n + \\alpha_i +\\epsilon_i\\\\ &amp;= \\mathbf{x}_i^T\\beta + \\alpha +\\epsilon \\\\ \\implies \\mathbf{Y} &amp;= \\mathbf{X}{\\beta} + {\\alpha} + \\epsilon \\end{aligned} \\] We can quantify the notion of what is considered “best” for a fitting polynomial by choices of distance norm, for example, the method of least squares finds the the vertical distances of data points to the polynomial through the Eucledian norm. Here, the residual vector, \\(\\vec{e}\\), represents error in calculation, so the optimalization problem is just for \\(\\beta\\), that is the mean squared error estimator. \\[\\hat\\alpha,\\hat\\beta= \\arg\\min_{\\alpha,\\beta} \\sum_{i=1}^n\\left(\\mathbf{Y}- (\\alpha+\\mathbf{X}\\beta) \\right)^2\\] \\[\\hat\\epsilon= \\mathbf{Y}- (\\hat\\alpha+\\mathbf{X}\\hat\\beta) \\] However, what about cases that the space of data lies on a Riemannian manifold such as a high dimensional sphere? Or if the projection of data from \\(\\mathbb{R}^n\\) on a manifold would better represent our data? By trying to find a polynomial lying on sub-dimensional space might be leading to over-fitting or resulted in a high variance result to determine. The key idea of geodesics regression is essential an analogy to linear regression, that is, finding the geodesics that fit data points lying on manifold domain the best. Here, the analogous version of a polynomial on \\(\\mathbb{R}^n\\) is an exponential map. 3.2 Geodesics Regression Definition (Exponential Map). Let \\(\\vec{\\mathbf{v}}\\in T_p\\mathcal{M}\\) be any vector in an arbitrary tangent plane of manifold \\(\\mathcal{M}\\), then there exists a unique geodesic \\(\\gamma_v\\) satisfying \\(\\gamma_v(0) = p\\) and \\(\\gamma&#39;_v(0) = \\vec{\\mathbf{v}}\\). The exponential map with respect to \\(\\vec{\\mathbf{v}}\\) is given by: \\[ \\begin{aligned} \\exp_p: T_p\\mathcal{M} &amp;\\longrightarrow \\mathcal{M}\\\\ \\vec{\\mathbf{v}} &amp;\\longmapsto \\gamma_v(1) \\end{aligned} \\] Note that the “exponential” part in its name is not explicitly reflected by the our usual reference to the exponent \\(e^x\\), we can simply think of it as a map that sends a vector from the tangent space of a point on the manifold to a geodesics on the manifold. The logarithm map \\(\\log_p: \\mathcal{M}\\to T_p\\mathcal{M}\\) is the inverse of the exponent map that sends a geodesics on a manifold to the tangent space. Figure 2.1: Tangent space at a point \\(p\\in\\mathcal{M}\\) with an exponential map visualized. Let’s re-define the notation \\(\\exp_p (\\vec{\\mathbf{v}}) = \\exp(p,\\vec{\\mathbf{v}})\\), then the regression module for \\(\\mathbf{X} \\subset \\mathbb{R}^n\\) is given by: \\[ \\begin{aligned} \\mathbf{y} = \\exp(\\exp(\\alpha,\\mathbf{X}\\beta),\\epsilon) \\end{aligned} \\] To think about this model, let’s us point out that an exponential map \\(\\exp(\\alpha,\\mathbf{X\\beta})\\) is simply the addition of vectors: \\(\\alpha+\\mathbf{X}\\beta\\). Then \\(\\exp(\\exp(\\alpha,\\mathbf{X}\\beta),\\epsilon) = \\alpha+\\mathbf{X}\\beta+\\epsilon\\), thus it’s the linear regression “version” on the Riemannian manifold! Figure 2.2: Geodesics regression on sample data lying on \\(S^2\\). 3.3 Least Square Estimate Let \\((\\mathbf{x_1}, \\mathbf{y_1}), (\\mathbf{x_2}, \\mathbf{y_2}),\\dots,(\\mathbf{x_n}, \\mathbf{y_n})\\) be a set of points on a Riemannian manifold \\(\\mathcal{M}\\) satisfying the geodesics model, \\(\\mathbb{E}[\\mathbf{y} \\vert \\mathbf{x}]= \\exp(\\exp(\\alpha,\\mathbf{x}\\beta),\\epsilon)\\), the least square estimate for \\(\\alpha\\) and \\(\\beta\\) is given by the optimalization problem: \\[\\hat\\alpha,\\hat\\beta = \\arg\\min_{\\alpha,\\beta} \\sum_{i=1}^n\\left(\\mathbf{Y}- \\exp(\\alpha,\\mathbf{X}\\beta)\\right)^2\\] Unlike linear regression, we cannot find a closed form solution for this problem, but we can approximate the solution with gradient descent. The predicted value for \\(\\mathbf{y}\\) and the residue are, respectively, given by: \\[ \\begin{aligned} \\mathbf{\\hat y}_i &amp;= \\exp(\\hat\\alpha,\\mathbf{x}_i\\hat\\beta)\\\\ \\hat\\epsilon_i &amp;= \\log(\\mathbf{\\hat y}_i,\\mathbf{y}_i) \\\\ \\end{aligned} \\] Figure 2.3: Geodesics regression on sample data lying on \\(S^2\\). Though, one of the important task to do after finding a geodesics fit model is to determine whehther this relationship is statistically significant. Firstly, the Frechet variance \\(\\sigma^2\\) of \\(\\{\\mathbf{y}_i\\}_{i=1,2,\\dots,n}\\) and of the residue terms are given by: \\[ \\begin{aligned} \\sigma^2 &amp;= \\frac{1}{n}\\min_{z\\in\\mathcal{M}}\\sum_{i=1}^n\\rho^2(z,\\mathbf{y}_i)\\\\ \\text{Var}[\\hat\\epsilon] &amp;= \\frac{1}{n}\\min_{z\\in\\mathcal{M}}\\sum_{i=1}^n\\rho^2(z,\\hat\\epsilon_i) \\end{aligned} \\] The coefficient of determination, \\(R^2\\), which denotes the proportion of global variance across \\(\\mathbf{y}_i\\) that can attributed by the geodesics regression with \\(\\mathbf{x}\\) is given bt: \\[R^2 = 1-\\frac{\\sigma^2}{\\text{Var}[\\hat\\epsilon]}\\] Figure 2.4: The residue plotted against increasing iterations of gradient descent. Example The \\(R^2\\) statistics of the geodesics regression on 100 sample points on \\(S^2\\) (figure 2.3) is 0.97. Then 97% of the variation in \\(\\mathbf{y}\\) is explained by the geodesics regression with 3% is attributed by other factors. From figure 2.4, we can see that the the residue factors are relatively low as we induced more iterations in gradient descent. Thus, this geodesics regression is a good fit for our data set. 3.4 Application In the original paper, Fletcher shows an application in medical image by projecting the shape of corpus callosum onto Kendall’s Shape Space. This is the complex projective space \\(\\mathbb{C}P^{k-2}\\) (analogous to the real projective space \\(\\mathbb{R}P^{k-1}\\)), which is a topological space constructed from removing the translation from the original space of data, and forming the quotient space from equivalence classes by identifying points that can be formed by scalings and rotations. Here, the shapes of the input lying on a high dimensional Riemannian manifold, which allows us to compute the geodesics regression and the mean shape! Figure 2.5: The sample shape of “heart-”shape Bezier Splines. Figure 2.6: The mean shape of “heart-”shape Bezier Splines sample. "],["conclusion.html", "Conclusion 3.5 References", " Conclusion As the topological structure of the Riemannian manifold, specifically its compactness, allows for well-behavaed statistical theory to hold, there are a variety of application of it in the field of computer vision. Though geometric comlexity of the Riemannian manifold was not being reflected clear in the scope of this project, it encompasses much more geometric invariant of vector space which makes it hold advantages over probability space on \\(\\mathbb{R}^n\\). The limitations of statistical methods on the Riemannian manifold in computer vision includes it being (1) computationally expensive and (2) requires careful analysis of camera angle’s effects on the data set. There are current developments that include both geometric and topological statistical methods to find extreme robust estimators. For example, Carlsson (2007) combines k-nearnest neighbor density estimation of images’ noise with persistence homology in order to find stable and robust features of image data set. That being said, the field of statistics on Riemannian manifold are still progressing to find more robust methodologies with more applications in other areas such as signal processing, geometric deep learning, cosmology, … 3.5 References Carlsson, G., Ishkhanov, T., De Silva, V., &amp; Zomorodian, A. (2007). On the local behavior of spaces of natural images. International Journal of Computer Vision, 76(1), 1-12. https://doi.org/10.1007/s11263-007-0056-x Fletcher, P. T., &amp; Zhang, M. (2016). Probabilistic geodesic models for regression and dimensionality reduction on riemannian manifolds. Riemannian Computing in Computer Vision, 101-121. https://doi.org/10.1007/978-3-319-22957-7_5 Gampat, C. (2018, March 6). High contrast vs low contrast black and white photo editing: What’s the difference? The Phoblographer. https://www.thephoblographer.com/2018/03/06/high-contrast-vs-low-contrast-black-and-white-photo-editing-whats-the-difference/ Henry, G., Munoz, A., &amp; Rodriguez1, D. (n.d.). k-Nearest neighbor density estimation on Riemannian Manifolds. https://arxiv.org/pdf/1106.4763.pdf Larsen, R. J., &amp; Marx, M. L. (2013). Introduction to mathematical statistics and its applications: Pearson new international edition. Pearson Higher Ed. Lee, A., Pedersen, K., &amp; Mumford, D. (2002). The Nonlinear Statistics of High-Contrast Patches in Natural Images. International Journal of Computer Vision. https://dash.harvard.edu/bitstream/handle/1/3637108/mumford_nonlinstatpatches.pdf?sequence=1 Nonparametric statistics on Hilbert manifolds. (2015). Nonparametric Statistics on Manifolds and Their Applications to Object Data Analysis, 297-304. https://doi.org/10.1201/b18969-16 Patrangenaru, V., &amp; Ellingson, L. (2015). Nonparametric statistics on manifolds and their applications to object data analysis. CRC Press. Pennec, X., Sommer, S., &amp; Fletcher, T. (2019). Riemannian geometric statistics in medical image analysis. Academic Press. Reinagel, P., &amp; Zador, A. M. (1999). Natural scene statistics at the centre of gaze. Network: Computation in Neural Systems, 10(4), 341-350. https://doi.org/10.1088/0954-898x_10_4_304 Sverdrup-Thygeson, H. (1981). Strong law of large numbers for measures of central tendency and dispersion of random variables in compact metric spaces. The Annals of Statistics, 9(1), 141-145. https://doi.org/10.1214/aos/1176345340 "]]
